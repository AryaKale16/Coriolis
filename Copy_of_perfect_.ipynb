{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlV7ig8K-KFu",
        "outputId": "5fa7469a-26dc-4742-b4fb-4ec316c78265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from PyPDF2) (4.5.0)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import PyPDF2  \n",
        "# import string\n",
        "\n",
        "# # Define the path to the input directory\n",
        "# input_dir = '/content/'\n",
        "\n",
        "# # Define the preprocess function\n",
        "# def preprocess(text):\n",
        "#     # Remove punctuation\n",
        "#     text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "#     # Lowercase the text\n",
        "#     text = text.lower()\n",
        "#     return text\n",
        "\n",
        "# # Loop over each file in the input directory\n",
        "# for filename in os.listdir(input_dir):\n",
        "#     if filename.endswith('.pdf'):\n",
        "#         # Define the path to the PDF file\n",
        "#         pdf_file_path = os.path.join(input_dir, filename)\n",
        "\n",
        "#         # Open the PDF file and create a PDF reader object\n",
        "#         with open(pdf_file_path, 'rb') as pdf_file:\n",
        "#             pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "#             num_pages = len(pdf_reader.pages)\n",
        "\n",
        "#             # Extract the text from each page of the PDF file\n",
        "#             text_list = []\n",
        "#             for i in range(num_pages):\n",
        "#                 page = pdf_reader.pages[i]\n",
        "#                 text_list.append(page.extract_text())\n",
        "\n",
        "#             # Preprocess the text and write it to the output file\n",
        "#             output_file_path = os.path.splitext(pdf_file_path)[0] + '.txt'\n",
        "#             with open(output_file_path, 'w') as output_file:\n",
        "#                 for text in text_list:\n",
        "#                     # Preprocess the text (e.g., remove punctuation, lowercasing, etc.)\n",
        "#                     preprocessed_text = preprocess(text)\n",
        "#                     # Write the preprocessed text to the output file\n",
        "#                     output_file.write(preprocessed_text + '\\n')\n",
        "import os\n",
        "import PyPDF2  \n",
        "import string\n",
        "\n",
        "# Define the path to the input directory\n",
        "input_dir = '/content/'\n",
        "\n",
        "# Define the preprocess function\n",
        "def preprocess(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Loop over each file in the input directory\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith('.pdf'):\n",
        "        # Define the path to the PDF file\n",
        "        pdf_file_path = os.path.join(input_dir, filename)\n",
        "\n",
        "        # Open the PDF file and create a PDF reader object\n",
        "        with open(pdf_file_path, 'rb') as pdf_file:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "            num_pages = len(pdf_reader.pages)\n",
        "\n",
        "            # Extract the text from each page of the PDF file\n",
        "            text_list = []\n",
        "            for i in range(num_pages):\n",
        "                page = pdf_reader.pages[i]\n",
        "                text_list.append(page.extract_text())\n",
        "\n",
        "            # Preprocess the text and write it to the output file\n",
        "            output_file_path = os.path.splitext(pdf_file_path)[0] + '.txt'\n",
        "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "                for text in text_list:\n",
        "                    # Preprocess the text (e.g., remove punctuation, lowercasing, etc.)\n",
        "                    preprocessed_text = preprocess(text)\n",
        "                    # Write the preprocessed text to the output file\n",
        "                    output_file.write(preprocessed_text + '\\n')\n"
      ],
      "metadata": {
        "id": "G3i-tyAu-P9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import nltk\n",
        "import os\n",
        "import PyPDF2\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "\n",
        "# Step 1: Collect the corpus\n",
        "corpus = []\n",
        "input_dir = '/content/'\n",
        "\n",
        "# Loop over each file in the input directory\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith('.txt'):\n",
        "        # Define the path to the text file\n",
        "        text_file_path = os.path.join(input_dir, filename)\n",
        "\n",
        "        # Open the text file and read its contents\n",
        "        with open(text_file_path, 'r') as text_file:\n",
        "            text = text_file.read()\n",
        "\n",
        "            # Append the preprocessed text to the corpus\n",
        "            corpus.append(preprocess(text))\n",
        "\n",
        "# Step 2: Preprocess the documents\n",
        "def preprocess(text):\n",
        "    # Tokenize the text into words\n",
        "    words = nltk.word_tokenize(text.lower())\n",
        "    \n",
        "    # Remove stop words and punctuation\n",
        "    words = [word for word in words if word.isalnum() and not word in stopwords.words('english')]\n",
        "    \n",
        "    # Stem the words\n",
        "    stemmer = nltk.stem.PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    \n",
        "    return words\n",
        "\n",
        "# Step 3: Calculate term frequencies\n",
        "def calculate_tf(document):\n",
        "    tf = defaultdict(int)\n",
        "    for word in document:\n",
        "        tf[word] += 1\n",
        "    return tf\n",
        "\n",
        "tf_corpus = [calculate_tf(document) for document in corpus]\n",
        "\n",
        "# Step 4: Calculate inverse document frequency (IDF)\n",
        "def calculate_idf(corpus):\n",
        "    N = len(corpus)\n",
        "    idf = defaultdict(float)\n",
        "    for document in corpus:\n",
        "        for word in document:\n",
        "            idf[word] += 1\n",
        "    \n",
        "    for word in idf:\n",
        "        idf[word] = math.log(N / idf[word])\n",
        "    \n",
        "    return idf\n",
        "\n",
        "idf = calculate_idf(corpus)\n",
        "\n",
        "# Step 5: Calculate document length\n",
        "def calculate_document_length(document):\n",
        "    length = 0\n",
        "    for word in document:\n",
        "        length += tf_corpus[corpus.index(document)][word] * idf[word] ** 2\n",
        "    return math.sqrt(length)\n",
        "\n",
        "document_lengths = [calculate_document_length(document) for document in corpus]\n",
        "\n",
        "# Step 6: Build the index\n",
        "index = defaultdict(list)\n",
        "for i, document in enumerate(corpus):\n",
        "    for word in set(document):\n",
        "        index[word].append((i, tf_corpus[i][word], idf[word]))\n",
        "\n",
        "# Step 7: Perform the query with BM25+RD scoring\n",
        "k1 = 1.2\n",
        "b = 0.75\n",
        "avg_doc_len = sum(document_lengths) / len(document_lengths)\n",
        "num_docs = len(corpus)\n",
        "doc_freqs = defaultdict(int)\n",
        "for word in index:\n",
        "    doc_freqs[word] = len(index[word])\n",
        "    \n",
        "def perform_query(query, idf, input_dir):\n",
        "    query = preprocess(query)\n",
        "    query_tf = calculate_tf(query)\n",
        "    query_idf = {word: idf[word] for word in query}\n",
        "    scores = defaultdict(float)\n",
        "    for word in query:\n",
        "        for document, tf, idf in index[word]:\n",
        "            idf = query_idf[word]\n",
        "            tf = tf_corpus[document][word]\n",
        "            doc_len = document_lengths[document]\n",
        "            doc_freq = doc_freqs[word]\n",
        "            score = idf * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avg_doc_len)))) * ((num_docs/2) / doc_freq)\n",
        "            scores[document] += score\n",
        "    ranked_documents = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    for i, result in enumerate(ranked_documents[:10]):\n",
        "        document_index = result[0]\n",
        "        score = result[1]\n",
        "        filename = os.listdir(input_dir)[document_index]\n",
        "        file_path = os.path.join(input_dir, filename)\n",
        "        print(f\"Rank {i+1}: Document {document_index} with score {score} and path {file_path}\")\n",
        "    return ranked_documents[:10]\n",
        "\n",
        "query = \"scheduling\"\n",
        "results = perform_query(query, idf, input_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRW7lj5NFv99",
        "outputId": "21888333-bbeb-4116-c496-05ed84022f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 1: Document 0 with score -6.764895511760308 and path /content/.config\n"
          ]
        }
      ]
    }
  ]
}